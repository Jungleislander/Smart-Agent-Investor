{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eb84f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ratios: {2: 0.3594864479315264, 0: 0.3238231098430813, 1: 0.3166904422253923}\n",
      "Class weights: tensor([0.9402, 1.1370, 0.9462], device='cuda:0')\n",
      "Epoch 5/50, Train Loss: 1.0907, Val Loss: 1.1094\n",
      "Epoch 10/50, Train Loss: 1.0757, Val Loss: 1.1140\n",
      "Epoch 15/50, Train Loss: 1.0697, Val Loss: 1.1180\n",
      "Epoch 20/50, Train Loss: 1.0625, Val Loss: 1.1248\n",
      "Epoch 25/50, Train Loss: 1.0592, Val Loss: 1.1288\n",
      "Epoch 30/50, Train Loss: 1.0514, Val Loss: 1.1304\n",
      "Epoch 35/50, Train Loss: 1.0559, Val Loss: 1.1312\n",
      "Epoch 40/50, Train Loss: 1.0542, Val Loss: 1.1313\n",
      "Epoch 45/50, Train Loss: 1.0549, Val Loss: 1.1315\n",
      "Epoch 50/50, Train Loss: 1.0502, Val Loss: 1.1317\n",
      "âœ… Model saved to eth_lstm_polygon.pth\n",
      "Prediction: Bearish\n",
      "Confidences -> Bearish: 0.398, Neutral: 0.313, Bullish: 0.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3.11.7\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Polygon.io ETH LSTM Pipeline (Optimized)\n",
    "# --------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from datetime import date, timedelta\n",
    "from polygon import RESTClient\n",
    "\n",
    "# --------------------------\n",
    "# USER SETTINGS\n",
    "# --------------------------\n",
    "API_KEY = \"peeXEfM2xJR2calDdtBPMB3RW3dp7KKA\"\n",
    "TICKER = \"X:ETHUSD\"\n",
    "\n",
    "SEQ_LEN = 150             # slightly shorter than 183 for more sequences\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "HIDDEN_SIZE = 40\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_SAVE_PATH = \"eth_lstm_polygon.pth\"\n",
    "SCALER_SAVE_PATH = \"eth_scaler_polygon.pkl\"\n",
    "\n",
    "# --------------------------\n",
    "# Polygon.io fetch last 2 years\n",
    "# --------------------------\n",
    "def fetch_polygon_last_2yrs(api_key, ticker):\n",
    "    client = RESTClient(api_key=api_key)\n",
    "    end_date = date.today()\n",
    "    start_date = end_date - timedelta(days=730)  # ~2 years\n",
    "    all_data = []\n",
    "    try:\n",
    "        for a in client.list_aggs(\n",
    "            ticker, 1, \"day\",\n",
    "            start_date.isoformat(), end_date.isoformat(),\n",
    "            limit=5000\n",
    "        ):\n",
    "            bar_date = (pd.to_datetime(a.timestamp, unit=\"ms\") + timedelta(days=1)).date()\n",
    "            all_data.append({\n",
    "                \"timestamp\": bar_date,\n",
    "                \"open\": round(a.open, 3),\n",
    "                \"high\": round(a.high, 3),\n",
    "                \"low\": round(a.low, 3),\n",
    "                \"close\": round(a.close, 3),\n",
    "                \"volume\": a.volume\n",
    "            })\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error fetching Polygon data: {e}\")\n",
    "    df = pd.DataFrame(all_data).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# --------------------------\n",
    "# Feature engineering\n",
    "# --------------------------\n",
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"return\"] = df[\"close\"].pct_change()\n",
    "    df[\"log_return\"] = np.log(df[\"close\"] / df[\"close\"].shift(1))\n",
    "    df[\"sma_7\"] = df[\"close\"].rolling(7).mean()\n",
    "    df[\"sma_30\"] = df[\"close\"].rolling(30).mean()\n",
    "    df[\"ema_14\"] = df[\"close\"].ewm(span=14).mean()\n",
    "    df[\"volatility_7\"] = df[\"return\"].rolling(7).std()\n",
    "    df[\"atr\"] = (df[\"high\"] - df[\"low\"]).rolling(14).mean()\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = np.where(delta>0, delta, 0)\n",
    "    loss = np.where(delta<0, -delta, 0)\n",
    "    avg_gain = pd.Series(gain).rolling(14).mean()\n",
    "    avg_loss = pd.Series(loss).rolling(14).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-9)\n",
    "    df[\"rsi\"] = 100 - (100 / (1 + rs))\n",
    "    ema12 = df[\"close\"].ewm(span=12).mean()\n",
    "    ema26 = df[\"close\"].ewm(span=26).mean()\n",
    "    df[\"macd\"] = ema12 - ema26\n",
    "    df[\"vol_change\"] = df[\"volume\"].pct_change()\n",
    "    df[\"obv\"] = (np.sign(df[\"return\"]) * df[\"volume\"]).cumsum()\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return df\n",
    "\n",
    "# --------------------------\n",
    "# Label generation\n",
    "# --------------------------\n",
    "def generate_next_day_labels(df, threshold=0.01):\n",
    "    df = df.copy()\n",
    "    df[\"return_next\"] = df[\"close\"].shift(-1) / df[\"close\"] - 1\n",
    "    df[\"label\"] = 1\n",
    "    df.loc[df[\"return_next\"] > threshold, \"label\"] = 2\n",
    "    df.loc[df[\"return_next\"] < -threshold, \"label\"] = 0\n",
    "    df[\"label\"] = df[\"label\"].fillna(1).astype(int)\n",
    "    ratios = df[\"label\"].value_counts(normalize=True).to_dict()\n",
    "    return df, ratios\n",
    "\n",
    "# --------------------------\n",
    "# Sequence creation with sliding window\n",
    "# --------------------------\n",
    "def create_sequences(df, features, target_col=\"label\", seq_len=SEQ_LEN, stride=1):\n",
    "    sequences, labels = [], []\n",
    "    data = df[features + [target_col]].values\n",
    "    for i in range(0, len(data) - seq_len, stride):\n",
    "        seq_x = data[i:i+seq_len, :-1]\n",
    "        seq_y = data[i+seq_len, -1]\n",
    "        sequences.append(seq_x)\n",
    "        labels.append(seq_y)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# --------------------------\n",
    "# Dataset class\n",
    "# --------------------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# --------------------------\n",
    "# LSTM Classifier\n",
    "# --------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS,\n",
    "                 num_classes=3, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "# --------------------------\n",
    "# Class weights helper\n",
    "# --------------------------\n",
    "def get_class_weights(y_train, num_classes=3, device=DEVICE):\n",
    "    unique_classes = np.unique(y_train).astype(int)\n",
    "    weights = np.ones(num_classes, dtype=np.float32)\n",
    "    computed = compute_class_weight(class_weight=\"balanced\", classes=unique_classes, y=y_train)\n",
    "    for i, cls in enumerate(unique_classes):\n",
    "        weights[int(cls)] = computed[i]\n",
    "    return torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# --------------------------\n",
    "# Training function\n",
    "# --------------------------\n",
    "def train_lstm_polygon(df, seq_len=SEQ_LEN, batch_size=BATCH_SIZE, epochs=EPOCHS):\n",
    "    df = add_features(df)\n",
    "    df, ratios = generate_next_day_labels(df)\n",
    "    print(\"Class ratios:\", ratios)\n",
    "\n",
    "    features = [c for c in df.columns if c not in [\"timestamp\",\"label\",\"return_next\"]]\n",
    "\n",
    "    X, y = create_sequences(df, features, seq_len=seq_len)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Scale\n",
    "    n_train, t, f = X_train.shape\n",
    "    n_val = X_val.shape[0]\n",
    "    scaler = StandardScaler()\n",
    "    X_train_2d = X_train.reshape(-1, f)\n",
    "    X_val_2d = X_val.reshape(-1, f)\n",
    "    X_train = scaler.fit_transform(X_train_2d).reshape(n_train, t, f)\n",
    "    X_val = scaler.transform(X_val_2d).reshape(n_val, t, f)\n",
    "    joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "\n",
    "    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = X.shape[2]\n",
    "    model = LSTMClassifier(input_size)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    class_weights = get_class_weights(y_train)\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_batch), y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                val_loss += criterion(model(X_batch), y_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"input_size\": input_size,\n",
    "        \"features\": features\n",
    "    }, MODEL_SAVE_PATH)\n",
    "    print(f\"âœ… Model saved to {MODEL_SAVE_PATH}\")\n",
    "    return model, features, scaler\n",
    "\n",
    "# --------------------------\n",
    "# Next-day prediction\n",
    "# --------------------------\n",
    "def predict_next_day(df, model, scaler, features, seq_len=SEQ_LEN):\n",
    "    df = add_features(df)\n",
    "    df = df[features].copy()\n",
    "    df[features] = scaler.transform(df[features])\n",
    "    if len(df) < seq_len:\n",
    "        raise ValueError(\"Not enough data for prediction\")\n",
    "    X_latest = torch.tensor(df.values[-seq_len:], dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(model(X_latest), dim=1).cpu().numpy()[0]\n",
    "        pred_class = int(np.argmax(probs))\n",
    "    return pred_class, probs\n",
    "\n",
    "# --------------------------\n",
    "# RUN: Fetch, Train & Predict\n",
    "# --------------------------\n",
    "df_polygon = fetch_polygon_last_2yrs(API_KEY, TICKER)\n",
    "model, features, scaler = train_lstm_polygon(df_polygon)\n",
    "\n",
    "pred_class, probs = predict_next_day(df_polygon, model, scaler, features)\n",
    "mapping = {0: \"Bearish\", 1: \"Neutral\", 2: \"Bullish\"}\n",
    "probs_percent = [f\"{p:.3f}\" for p in probs]\n",
    "print(f\"Prediction: {mapping[pred_class]}\")\n",
    "print(f\"Confidences -> Bearish: {probs_percent[0]}, Neutral: {probs_percent[1]}, Bullish: {probs_percent[2]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
